#aws [[AWS 1]] [[Data Engineering]]

A pipeline includes processes for ingestion, optimization and transformation of data for end consumers to use for insights.

![[Screenshot_2024-07-17-09-15-17-36_45fb746795678a1ded33faef3c58e9c7.jpg]]![[Screenshot_2024-07-17-09-27-07-30_45fb746795678a1ded33faef3c58e9c7.jpg]]
# Ingestion 

Considerations: 
* Sources (internal/external )
* Owner, access
* Frequency (batch/streaming )
* Format 
* Required governance controls for PII 
* Potential tools

![[Screenshot_2024-07-19-08-59-16-53_45fb746795678a1ded33faef3c58e9c7.jpg]]

Transformations:
* File format optimization 
* Standardization: column names, types and formats.
* Quality checks
* Data partitioning: partition helps to group data at storage layer. The partitions are stored in the same S3 prefix.
* Data denormalization.
* Cataloging (technical/business )

![[Screenshot_2024-07-20-09-51-22-64_45fb746795678a1ded33faef3c58e9c7.jpg]]

![[Screenshot_2024-07-20-11-37-40-85_45fb746795678a1ded33faef3c58e9c7.jpg]]

![[Screenshot_2024-07-20-12-28-28-70_45fb746795678a1ded33faef3c58e9c7.jpg]]

# Ingestion 

* 5 V's of data 
	* Variety
		* Structured
		* Unstructured 
		* Semi structured
			* Each record may contain different number of fields
	* Volume
		* Points to consider: size of existing data as well as growth
	* Veracity 
		* Quality 
		* Completeness 
		* Accuracy 
	* Velocity 
		* Ingestion frequency for batch processing 
		* 
	* Value (business value)


## Considerations: 

* Data format
	* Structured
	* Semi structured 
	* Unstructured 
* Size of historical data 
* Batch or streaming?
* Ingestion frequency 

## Ingestion from relational database 

### DMS 

Applications:
* One off ingestion of historical data from a database 
* Replication of change data (Change Data Capture (CDC)) on an ongoing basis. 

Source: database
Target: different database or S3 based data lake.

## Glue

Glue has jdbc and other connectors for various data sources.

It can take up ingestion and transformations.

### Full one off loads: 
* It dumps everything from a table into memory of Spark cluster. We can dump this data into S3, taking up file format optimization along the way.
### Initial full loads and subsequent loads of new records

Glue has a concept of job bookmarks which lets it track data it has processed and on subsequent run process only new data. It treats one of the columns as a bookmark key which needs to be monotonous increasing. This process can't handle CDC as the table changes. 

### Using Glue jobs with Lake Formation 

Lake Formation templates can be used to deploy Glue for ingestion.

Lake Formation helps us with ingestion, crawlers and orchestrating Glue components.

### Other ways to ingest

* Using Spark on EMR with a jdbc connector to link up with a relational database.
* Export data from RDS to data lake.


When rows in a relational database are deleted or updated, there is no practical way to capture those changes using standard database query tools (such as SQL). But when replicating data from a database to a new source, it is important to be able to identify those changes so that they can be applied to the target. This process of identifying and capturing these changes (new inserts, updates, and deletes) from the database log files is referred to as CDC.

Ingestion thumb rule:
	* For small dataset, use RDS export as snapshot if your db supports it.
	* For large datasets, use snow or read replica to ingest and then use DMS to apply CDC.

## Ingesting streaming data 

* Kinesis vs MSK (Managed Streaming for Kafka)
	* Both services have stream producers decoupled from data consumers. Data is first written to storage and then that stream is consumed.

### Serverless vs Managed services
* Kinesis flavors
	* Provisioned 
		* We need to configure number of shards.
		* A shard is the basic unit of a stream throughput.
	* On demand
		* The number of shards is managed by Kinesis.
	* In both modes Kinesis manages the underlying resources (EC2/EBS) automatically. What we decide is how to manage shards (provisioned/managed).
	* For both firehose and data streams on demand, the scaling up and down happens automatically.
* MSK Flavors 
	* Provisioned 
		* You need to decide on nature of underlying resources such as type of EC2 instance, VPC, storage size and options, Kafka version and it's settings.
	* On demand (serverless)
		* Everything is taken care of by AWS.

## Atleast once messaging vs Exactly once messaging 
* Relates to message processing by data consumers.
* Kinesis
	* Atleast once message processing guarantee. This ensures that every message generated by the producer will be delivered to consumer for processing.
	* Introduces the possibility of data duplication. This requires additional handling of duplicates.
* MSK 
	* Exactly once messaging is supported.
	* 

# Cloud Formation 

It's a service that lets us deploy services through code (infrastructure via code). We can use yaml or json for this.

We can build ci/cd pipelines and add version control to it.


* Streaming data can be generated using KDG (kinesis Data generator)


# Transformations 

* File format optimization 
* Incorporation of business logic into data
* Includes joining multiple datasets.

## Tools
* Spark using Glue, EMR, ECS, EKS
* Hadoop and MapReduce using EMR
* GUI based tools
	* Some tools can also generate the transformation code
	* Glue Databrew 
	* Glue Studio

## Common Data Transformations 

* Protecting PII data: both glue Studio and Databrew can be used to anonymize PII data.
* File format optimization
* Optimising with data partitioning 
	* Determines how data files are organised in storage.
	* Table can be grouped and stored in various folders basis one or more columns.
	* There could be multiple files per partition.
	* Optimal size of parquet files: between 128 MB and 1 GB. A single file can also be split by multiple nodes.
	* It is better to have fewer partitions with larger files than too many partitions with very small files as this saves overhead of opening many files.

## Data Cleansing

* This is done to ensure data is valid, accurate, consistent, complete and uniform.
* Scenarios handled:
	* Missing values 
	* Duplicates 
	* Data formats uniformity
	* Inconsistent column names
* After cleansing we may apply CDC to the data.


## Common business use case transformations 


* We can combine datasets, enrich, denormalize them to add value.
* Table joining can be done using Spark, SQL or glue/glue Studio.
* Creating aggregates
* Extracting metadata from unstructured data

## Working with CDC

* CDC: change data capture 
* This includes processing updates to existing data. 
* CDC can be indicated though a column in the file with the following values: 
	* I : insert : new data
	* U : update: this record updates an existing record
	* D : delete: contains data for a record that was deleted from the table.

### Approaches to handle CDC in data Lake:
* Traditional approaches: data upserts and SQL views 
	* Run a scheduled transform job that applies changes to existing dataset, keeping only the latest records. This is called upsert (update and insert).
	* This may end up using custom logic.
	* We can either overwrite the existing data or write to a new date based partition effectively creating a new snapshot of the data.
	* This is also operationally challenging in terms of dataset size and parallel consumption of data during updates.
	* DMS can be used to capture CDC and then Glue can be used to apply it to the dataset. Dynamo db can be used to store a config table that contains data on which columns to use as primary key and which ones to use for partitioning. CloudFormation can be used to apply the steps to automate CDC.
	* 
* Modern approach: Open Table Formats (OTF)
	* OTFs allows us to build transactional data lakes.
	* These also allow time travel (ability to query data as it was at a point in time) and schema evolution.
	* OTFs bring ACID semantics to the data Lake
		* Atomicity: either the data is written in full or not written at all. 
		* Consistency: even if a failure happens, the dataset remains consistent.
		* Isolation: one transaction will not affect another being requested at the same time.
		* Durability: once a transaction has been successfully completed, this transaction will be durable (permanent, even in the case of a later failure).
#### Apache Iceberg 
* Supports schema evolution, time travel, atomic changes, support for multiple simultaneous writers.
* Glue supports native support for Iceberg.

#### Apache Hudi

* Supported by AWS through Glue/EMR 

#### Databricks Delta Lake

* Supports both time travel and acid.



# Datamarts and Redshift [[Redshift]]

* Datamarts are expected to be a subset of the data which can be classified into
	* Cold
		* Required to be stored for compliance reasons or ML development but not actively queried.
	* Warm
		* Queried often but does not require extremely low retrieval latency.
		* Could include landing zone data that may be queried on a regular basis. Such data can be batch processed with business transforms and moved to curated zone.
		* With the Infrequent Access storage classes, you pay a lower per-GB cost than S3 Standard for storing data, but there is a per-GB charge for reading/accessing the data. Therefore, this storage class is ideal for data that you need to store for longer periods of time and access infrequently.
		* Note that data stored in the Infrequent Access storage classes is always charged for a minimum of 30 days, therefore it is not suited to data that will be deleted in less than 30 days. Infrequent Access is intended for longer-term data storage.
		* The Glacier Instant Retrieval storage class has a lower cost per GB of data stored than the Standard and Infrequent Access classes, but a higher cost to retrieve/access data than the other classes.Objects in the Glacier Instant Retrieval storage class do not need to be restored before you access them, and therefore they can be queried by Athena.
		* The Amazon S3 Glacier Flexible Retrieval storage class is intended for long-term storage where access to the data may be required a few times a year, and immediate access is not required. Data can be retrieved from S3 Glacier in minutes to hours (with different price points for the retrieval, based on how quickly the data is required). Data in S3 Glacier cannot be directly queried with Amazon Athena or Glue jobs – it must be retrieved and stored in a regular storage class before it can be queried.
		* The Amazon S3 Glacier Deep Archive storage class is the lowest-cost storage for long-term data retention and is intended for data that may be retrieved at most once or twice a year. Data in this storage class can be retrieved within 12 hours.
		* Storage charges for objects in S3 Glacier storage classes are for a minimum of 90 days for Instant Retrieval and Flexible Retrieval and 180 days for Deep Archive, so they are not intended for data that is short-lived.
		* Intelligent tiering: The default tier is the Frequent Access tier, but if an object in this tier is not accessed for 30 days, it is automatically moved to the Infrequent Access tier. If an object is not accessed for 90 consecutive days, it is automatically moved to the Archive Instant Access tier (which, much like Glacier Instant Retrieval, makes the object available in milliseconds). You can optionally also enable having the object moved to the Archive Access tier, after anywhere between 90 and 730 days without it being accessed. Once data is moved to the Archive Access tier, it must be retrieved before it can be accessed, and retrieval ranges from minutes to 5 hours, although you can request expedited retrieval (at an additional charge), which restores objects in 1–5 minutes. An additional option is to activate the Deep Archive Access tier for data that has not been accessed for between 180 and 730 days (configurable). The Deep Archive Access tier is similar to the Glacier Deep Archive storage class in terms of time to retrieve objects (9–12 hours).When an object that has been moved into an Infrequent Access or Archive tier is accessed, it is moved back into the Frequent Access tier.
	* Hot

* DW is mostly designed only for append or insert queries and not updates.
* The goal is to avoid storing unnecessary data in a data warehouse. Data warehouses are supposed to store curated datasets with well-defined schemas and should only store hot data that is needed for high-performance, low-latency queries.

## Architecture 

* Built of compute and leader nodes.
* Each compute node has compute and storage attached to it.
* For fault tolerance each compute node has 2.5-3x stated storage capacity.
* Each compute node is split into 2,4 or 16 slices. The slices enable parallel execution on one or more parts of the data.
* Since Redshift is columnar, each slice stores certain columns of the data as decided by leader node.
* The data for each column is stored in 1 MB immutable blocks.
![[Screenshot_2024-08-21-09-10-54-79_45fb746795678a1ded33faef3c58e9c7.jpg]]

* Distribution styles are used to spread the data across slices to improve query performance. Supported styles are KEY, EVEN, ALL. This determines how rows for various columns are stored across slices.
* Dimension tables can be stored using ALL style. This saves it on all slices and improves join performance with fact table at each slice since dim table doesn't need to shifted across slices. Fact table may be stored using EVEN.
* An alternative approach that can be used to optimize joins, especially if both tables being joined are large, is to ensure that the same slice stores the rows for both tables that will need to be joined. A way to achieve this is by using the KEY distribution style, where a hash value of one of the columns will determine which row of each table will be stored on which slice.
* For grouping and aggregation queries, you also want to reduce data shuffling (copying data from one node to another to run a specific query) to save network I/O. This can also be achieved by using the KEY distribution style to keep records with the same key on the same slice. In this scenario, you would specify the column used in the GROUP BY clause as the key to distribute the data on.
* However, if we queried one of these tables with a WHERE filter on the product_id column, then this distribution would create a bottleneck, as all the data that needed to be returned from the query would be on one slice. As such, you should avoid specifying a KEY distribution on a column that is commonly used in a WHERE clause.


## Redshift Zone Maps and sorting data

* Zone map is essentially metadata about each disk block, stored on leader node.
* They become more effective if the data on the blocks is sorted.
* When choosing multiple sort keys, you can either have a priority order of keys using a compound sort key or give equal priority to each sort key using an interleaved sort key. The default sort key type is a compound sort key, and this is recommended for most scenarios.
* Sort keys should be on columns that are frequently used with range filters or columns where you regularly compute aggregations.

## Provisioned versus serverless clusters

* For provisioned clusters we need to specify number and type of nodes. 
* For serverless cluster we need to specify number of Redshift Processing Units (RPU). This needs to be base capacity and Redshift will autoscale on need basis.
* Since for serverless costs can become unpredictable, you can set a threshold for maximum rpu as well.
* Redshift Serverless clusters also do not require you to specify maintenance windows or to plan for software upgrades. 

## Redshift Node Types
* Node type impacts compute capacity, storage capacity and storage type.
* RA3 node:
	* Compute and storage are de-coupled. This means that both can be scaled independent of each other.
	* Compute: pay per hour
	* Storage: how much managed storage is used over a month
	* Storage: local SSD + Redshift managed S3
	* Recommended for larger DW (>1 TB)
	* Only node type that supports data sharing
	* 
* DC2 node:
	* Designed for compute intensive workloads
	* Fixed SSD per node.
	* Storage and compute are coupled.
* DS2 node:
	* Legacy
	* Compute attached with large hard disk drive.
	* Compute and storage are coupled here.

## Distkey and Sortkey

* If you create a new table and do not specify a specific distribution style or sort key, Redshift sets both of those settings to AUTO. Smaller tables will initially be set to have an ALL distribution style, while larger tables will have an EVEN distribution style. If a table starts small but grows over time, Redshift automatically adjusts the distribution style to EVEN.

## Redshift Table types
* Local tables
	* Persisted on storage managed by Redshift (SSD+S3).
	* For RA3 nodes Redshift stores data on S3 and loads frequently queried data to local storage for performance enhancement. This structure corresponds to Redshift Managed Storage (RMS).
	* Data movement between S3 and SSD is determined basis block temperature, block age, workload and data access patterns.
* External tables (spectrum)
	* These basically point to objects in Glue Data Catalog and correspond to database objects.
	* We basically create the schema for the catalog item and then spectrum can be used to query the tables.
	* This is a hybrid version that allows us to leverage Massively Parallel Processing (MPP) architecture of Redshift but over S3 files.
	* Spectrum also supports Open Table Formats such as Hudi, Iceberg, Delta lake.
* Temp tables
	* These do not trigger incremental backups in Redshift.
	* These can be merged with the dataset to update a particular record.


## Optimising data Ingestion in Redshift 

* The recommended way to ingest data is using Redshift copy command.
* It supports Ingestion from S3, DDB, EMR, Remote SSH hosts.
* To take advantage of the multiple compute nodes in a cluster when ingesting files into a Redshift-provisioned cluster, you should aim to match the number of ingest files with the number of slices in the cluster. Each slice of the cluster can ingest data in parallel with all the other slices in the cluster, so matching the number of files to the number of slices results in the maximum performance for the ingest operation.
* If you have one large ingest file, it should be split into multiple files, with each file having a size between 1 MB and 1 GB (after compression). To determine how many slices you have in your cluster, refer to the AWS documentation on Redshift cluster configuration. For example, if you had a cluster with 4 x ra3.4xlarge nodes, you would have 16 slices (there are four slices per ra3.4xlarge node). If your ingest file were 64 GB in size, you would split the file into 64 x 1 GB files, and each of the slices in the cluster would then ingest a total of four files.
* Note that when using the COPY command to ingest data, the COPY operation is treated as a single transaction across all files. If one of our 64 files failed to be copied, the entire copy would be aborted and the transaction would be rolled back.


# Orchestration 
* Pipeline steps can either be scheduled or run in response to a trigger.
* While you could place all of the steps of a process in a single data pipeline, it is a recommended best practice to split pipelines into the smallest logical grouping of steps.
* Some tools such as Airflow require the pipelines to be DAG. Some other tools do support it, like AWS Step Functions.
* Pipelines:
	* Scheduled based
	* Trigger/event based

## Using manifest files as manifest triggers

* Manifest file contains information about other files that are a part of a batch of files.
* A manifest file can be used to verify that all files in the desired batch have been received and the pipeline can be run on the set of files.

## Handling failures at any step in the pipeline 

* It's critical to have log files for each step.
* Failure reasons
	* Data quality issues: can lead to hard failures. These are the ones that the job cannot recover from until the issue has been fixed.
	* Code errors: hard failures.
	* Endpoint errors: this is caused by connection errors to source/sink of data. This could be a network failure (soft failure) or insufficient permissions (hard failure)
	* Dependency errors: these can be hard failures (if upstream has hard failures) or soft failures. Soft failures can benefit from having a good retry strategy.
* Pipeline failure retry strategies:
	* Airflow and Step Functions allow us to specify the number of retries, interval between retries and backoff rate.
	* Retry backoff rate (also called exponential backoff) causes the interval betwen retries to increase exponentially.

## Options for orchestration

* Serverless tools: 
	* AWS Data Pipeline: now in maintenance mode
	* Step Functions 
	* MWAA: Amazon Managed Workloads for Apache Airflow 
	* Glue Workflows
* When making a decision on this, there are multiple factors to consider, such as the level of management effort, the ease of integration with your target ETL engine, logging, error-handling mechanisms, cost, and platform independence.

### Glue Workflows for Glue resources [[Glue]]

* Can be used to orchestrate glue components (crawlers and etl jobs).
* Can be used if the pipeline uses only Glue components.
* You can use boto3 on glue python shell to use with other services.
* Monitoring and error handling
	* UI for monitoring job progress.
	* With the UI you can check whether any step in the pipeline has failed and you can resume from a specific step.
	* Workflow does not have retry mechanism but individual jobs can have retries configured.
	* Cloudwatch provides a stream of change events. Individual glue jobs can generate cloudwatch logs. EventBridge can be used to automate actions when new events and job status are generated.
* Triggering Glue Workflows 
	* On demand
		* Manual job start, Glue API or CLI.
	* Scheduled 
		* Also supports cron expressions.
	* Event driven
		* Basis EventBridge events which can send events to Glue Workflow as the target.
		* This also supports buffering of events (number of events or amount of time).
### Airflow [[Airflow]]

* It has inbuilt functionality for monitoring, logging, alerting.
* Code based approach for orchestration (python)
* MWAA is a managed service for Airflow.
* Airflow creates the pipeline as a DAG, using Python to define the DAG. The DAG defines the pipeline steps and dependencies between the tasks.
* Airflow Tasks:
	* A task is the basic unit of work that a DAG performs.
	* Each task has upstream and downstream dependencies which defines the execution sequence.
	* When a DAG runs, the tasks in the DAG move through various states, from None to Scheduled, to Queued, to Running, and then to Success or Failed.
* Airflow Hooks
	* These define how to connect to remote source and target systems.
	* Contains the code that define the connection. This keeps the connection code separate from the pipeline definitions.
* Airflow Operators
	* These are pre defined task templates for specific tasks.
	* These can include operators for executing bash operators and python functions as well.
* Airflow Sensors
	* It is a special operator that allows for waiting until a specific action is completed and then trigger a specific DAG.
	* These can also be configured to time out after a certain period.
	* These help us build event driven pipelines.
* Airflow Connections 
	* Define the remote connection configuration.
	* Can be used by hooks, sensors and operators to define the authentication credentials for external systems such as S3, RDS, lambda etc.
* Since MWAA is a managed service there is a fixed infrastructure cost for the service.

### AWS Step Functions [[Step Functions]]


* Enables serverless pipelines.
* Visual process. Alternate: define the process using Amazon States Language (ASL) directly using JSON.
* Provides support for custom retry policies, can define catch block for specific errors and can define the action basis the error.
* Do not support restarting the state machine from a specific step.
* The state machine describes the various tasks as states.
* Choice states manipulate flow of pipeline between states and executes a branch of a pipeline.
* A wait state pauses the execution for a defined period.
* Each state gets a payload to act upon, on which it can also add additional metadata such as status code.
* The visual editor also generates an ASL file which can be setup as part of CI/CD flow.


Event bridge hands on