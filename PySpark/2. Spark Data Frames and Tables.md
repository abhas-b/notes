
#spark 

[[Spark]]


Notes about a general database:
* data in a table stored as a .dbf file (stored on the disk)
* Table: 
	* schema : list of column names and data types
		* stored in a database data dictionary or a metadata store
	* data
* The three layers to form a table:
	* Physical Storage layer (hard disk)
	* Logical layer (the table itself, which is queried)
	* Metadata Layer (schema)


Spark has:
* Spark Database and SQL
	* Instead of .dbf files, we can choose from many file format options such as json, parquet, csv, AVRO, XML
	* Storage layer supports distributed storage (HDFS, S3, ADLS)
	* Also has a metadata store
* Spark Dataframe and API
	* A dataframe is a table without a metadata store.
	* It has a runtime metadata catalog to store schema info (created at runtime). It is deleted once the application terminates.
* A dataframe is a run-time in-memory object. In contrast, a table has persistence (until dropped).
	*  It supports schema on read. This means that we get to define the schema when we are creating the df. Then the df is loaded with the provided data.
	* Spark dataframes are immutable
* Sparksession is our entry point for the APIs.

Spark Table
* Store schema information in metadata store.
* Created with pre-defined schema.
* Table and metadata are persistent objects and visible across applications.
* There is no delete and update command in spark sql.

Databases:
* Physical Storage layer
	* Files on hard disk
* Logical Layer 
	* DB Table
* Metadata layer
	* Table Schema
	* The schema is stored in a Meta Store.



Spark offers two ways of processing data:
* Spark Database and SQL
	* The physical files supported need not be only dbf files (as is the case for SQL databases.)
	* The formats supported are csv, parquet, avro, xml.
	* It also supports distributed storage such as HDFS and cloud storage such as S3, ADLS.
	* The schema is stored in metadata store.
* Spark Dataframe and API
	* The dataframe is structurally the same as a database. But it does not store any information in the metadata store.
	* Instead it uses a runtime metadata catalog to store schema information.
* Key difference between database and dataframe:
	* Dataframes are runtime objects. Once your application terminates the dataframe is gone. On the other hand database tables are permanent. The data frames live in memory. Database lives on disk.
	* Dataframe supports schema on read. Due to this the dataframe will always load the data whereas a table can be empty.
	* Tables support SQL expressions and does not support API whereas dataframes support API and not SQL expressions.





File reading shortcut method: [[Spark Commands]] [[Ingestion]]
```
spark.read.csv
spark.read.parquet
spark.read.jdbc
spark.read.orc
spark.read.json
spark.read.table
```


#### Convert dataframe to a table like view [[Spark Commands]] [[Ingestion]]
```
fire_df.createGlobalTempView("fire_service_calls_view")
query = """select * from global_temp.fire_service_calls_view limit 10"""  
spark.sql(query)
```

#### Create database and table [[Spark Commands]] [[Ingestion]]

```
create database if not exists demo_db;
create table if not exists demo_db.fire_service_calls_tbl (
	callnumber integer,
	unitid string
) using parquet;
insert into demo_db.fire_service_calls_tbl 
	select * from global_temp.fire_service_calls_view


```

* Spark does not offer a delete statement. You cannot delete data from a Spark Table using Spark SQL. You can use truncate statement for the same.
* 