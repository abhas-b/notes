#spark 

[[Spark]]

## Introduction

* Data Categories:
	* Structured : row column structure
	* Semi-Structured : does not have row-column structure but follows well defined key-value structure (json, xml)
	* Unstructured: pdf, text files, images, videos etc

* Bid Data Problem
	* Variety (refer Data categories above)
	* Volume
	* Velocity

* Scalability
	* Vertical: add more resources to the single / monolithic system in a vertical manner.
	* Horizontal: add more elements to the cluster for scaling. This increases the length of the network.

* Hadoop Core Layer Components: [[Hadoop Fundamentals]]
	* YARN : cluster operating system (official name: Hadoop cluster resource manager)
		* Decides how to share cluster resources between applications.
		* Components:
			* RM : Resource Manager
				* Installed on Master Node
				* Receives the application request from the client.
			* AM : Application Master
				* Runs on worker nodes.
			* NM: Node Manager
				* Installed on worker nodes.
				* Sends regular node status report to RM.
		* After receiving application request from the client, resource manager will request one of the nodes (NM) to start a resource container and run an AM. The application starts inside the AM container.
			* Each application on YARN runs inside a different container.
	* HDFS: distributed storage
		* Components:
			* NN : Name node
				* installed on the Master Node
				* it breaks the target file into smaller parts called blocks (with size such as 128 MB). Name node keeps track of all file metadata 
					* file name
					* storage location
					* block size
					* how many blocks are there
					* block ID
					* block sequence
					* block location
			* DN : Data Node
				* installed on the workers
	* Map/Reduce : distributed computing
		* It is a programming model
			* a technique of solving problems
		* It is also a framework
			* a set of APIs/services that allow us to use map reduce.
		* Reduce is started at a particular data node in a new container, and all other data nodes send it their results from map function. The new container starts once all map functions are done processing.
		* Map: parallel processing. Reduce: aggregations.
* Spark and Hadoop
	* Spark can be used with Hadoop (Data Lake)
	* Spark can also be used without Hadoop (Lakehouse) (cloud) (Databricks)



## What is Spark?

* It is a distributed data processing framework.
* Spark Core
	* Spark Engine
	* APIs
* It does not offer Cluster Management and Storage Management.
	* These are done by YARN, Kubernetes, Mesos
	* HDFS, S3, Azure Blob, GCS, CFS (Cassandra)
* Initially the APIs were based on RDD (Resilient Distributed Datasets).
* 

